{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8SHhD9vrS66jFBomKtUqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafapecino/6-2-23/blob/main/ExtraccionCVS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install PyPDF2\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEFL880ZzWeD",
        "outputId": "6b5678af-9bae-4a60-a4f0-abeb0ba5e03e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import PyPDF2\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "class CVInfoExtractor:\n",
        "    def __init__(self):\n",
        "        self.section_patterns = {\n",
        "            \"datos_personales\": [\n",
        "                r\"datos\\s+personales\", r\"contacto\", r\"perfil\\s+personal\", r\"información\\s+personal\",\n",
        "                r\"sobre\\s+mí\", r\"fecha\\s+de\\s+nacimiento\", r\"dirección\", r\"teléfono\", r\"correo\", r\"email\"\n",
        "            ],\n",
        "            \"educacion\": [\n",
        "                r\"educación\", r\"formación\\s+académica\", r\"datos\\s+académicos\", r\"estudios\",\n",
        "                r\"titulación\", r\"grado\", r\"ciclo\\s+formativo\", r\"centro\\s+de\\s+formación\",\n",
        "                r\"universidad\", r\"bachillerato\", r\"licenciatura\", r\"master\", r\"doctorado\"\n",
        "            ],\n",
        "            \"experiencia\": [\n",
        "                r\"experiencia\\s+laboral\", r\"experiencia\\s+profesional\", r\"historial\\s+laboral\",\n",
        "                r\"trayectoria\\s+profesional\", r\"trabajo\", r\"empleos\", r\"prácticas\", r\"formación\\s+en\\s+centros\"\n",
        "            ],\n",
        "            \"habilidades\": [\n",
        "                r\"habilidades\", r\"competencias\", r\"conocimientos\", r\"aptitudes\", r\"destrezas\", r\"capacidades\",\n",
        "                r\"conocimientos\\s+técnicos\", r\"herramientas\", r\"programas\", r\"actitudes\"\n",
        "            ],\n",
        "            \"idiomas\": [\n",
        "                r\"idiomas\", r\"lenguajes\", r\"nivel\\s+de\\s+idioma\", r\"ingles\", r\"inglés\", r\"español\", r\"castellano\",\n",
        "                r\"nativo\", r\"b1\", r\"b2\", r\"c1\", r\"c2\", r\"francés\", r\"alemán\", r\"italiano\", r\"portugués\"\n",
        "            ],\n",
        "            \"certificaciones\": [\n",
        "                r\"certificaciones\", r\"certificados\", r\"cursos\", r\"curso\\s+de\", r\"diplomas\", r\"acreditaciones\",\n",
        "                r\"especialización\", r\"diploma\"\n",
        "            ],\n",
        "            \"otros\": [\n",
        "                r\"hobbies\", r\"aficiones\", r\"intereses\", r\"voluntariado\", r\"referencias\",\n",
        "                r\"disponibilidad\", r\"carnet\", r\"permiso\\s+de\\s+conducir\", r\"carnet\\s+de\\s+conducir\", r\"coche\\s+propio\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                return \" \".join(page.extract_text() or \"\" for page in reader.pages) # Corrected indentation and logic\n",
        "        except Exception as e:\n",
        "            print(f\"Error al procesar el PDF {pdf_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return re.sub(r'[^\\w\\s\\d\\-áéíóúÁÉÍÓÚñÑüÜ@.,:;+()/%]', '', text)\n",
        "\n",
        "    def split_into_sections(self, text):\n",
        "        section_headers = [\n",
        "            r'\\b(?:DATOS PERSONALES|PERFIL|SOBRE MÍ)\\b',\n",
        "            r'\\b(?:EDUCACIÓN|FORMACIÓN|ESTUDIOS|DATOS ACADÉMICOS)\\b',\n",
        "            r'\\b(?:EXPERIENCIA|HISTORIA LABORAL|EMPLEOS|PRÁCTICAS)\\b',\n",
        "            r'\\b(?:HABILIDADES|COMPETENCIAS|CONOCIMIENTOS|APTITUDES)\\b',\n",
        "            r'\\b(?:IDIOMAS|LENGUAJES)\\b',\n",
        "            r'\\b(?:CERTIFICACIONES|CURSOS|DIPLOMAS)\\b',\n",
        "            r'\\b(?:INFORMACIÓN ADICIONAL|OTROS|AFICIONES|REFERENCIAS)\\b'\n",
        "        ]\n",
        "        markers = []\n",
        "        for pattern in section_headers:\n",
        "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
        "                markers.append((match.start(), match.group()))\n",
        "        markers.sort()\n",
        "        sections = []\n",
        "        for i in range(len(markers)):\n",
        "            start = markers[i][0]\n",
        "            end = markers[i+1][0] if i < len(markers) - 1 else len(text)\n",
        "            sections.append((text[start:end].strip(), start, end))\n",
        "        if not sections:\n",
        "            sections.append((text, 0, len(text)))\n",
        "        return sections\n",
        "\n",
        "    def classify_section(self, section_text):\n",
        "        scores = {}\n",
        "        section_lower = section_text.lower()\n",
        "        for category, patterns in self.section_patterns.items():\n",
        "            scores[category] = sum(len(re.findall(p, section_lower, re.IGNORECASE)) for p in patterns)\n",
        "        best_category = max(scores.items(), key=lambda x: x[1])\n",
        "        if best_category[1] == 0:\n",
        "            if re.search(r'\\d{4}-\\d{4}|\\d{4} - \\d{4}|\\d{2}/\\d{2}/\\d{4}', section_lower):\n",
        "                if re.search(r'universidad|colegio|escuela ', section_lower):\n",
        "                    return \"educacion\", 1\n",
        "                return \"experiencia\", 1\n",
        "            if re.search(r'python|java|c\\+\\+|html|css|javascript', section_lower):\n",
        "                return \"habilidades\", 1\n",
        "            if re.search(r'inglés|español|francés|alemán', section_lower):\n",
        "                return \"idiomas\", 1\n",
        "            return \"otros\", 0\n",
        "        return best_category[0], best_category[1]\n",
        "\n",
        "    def extract_structured_info(self, text):\n",
        "        processed_text = self.preprocess_text(text)\n",
        "        sections = self.split_into_sections(processed_text)\n",
        "        categorized_sections = {}\n",
        "        for section_text, _, _ in sections:\n",
        "            category, _ = self.classify_section(section_text)\n",
        "            categorized_sections.setdefault(category, []).append(section_text)\n",
        "        result = {}\n",
        "        for category in self.section_patterns.keys():\n",
        "            result[category] = \"\\n\\n\".join(categorized_sections.get(category, [\"No se encontró información\"]))\n",
        "        return result\n",
        "\n",
        "    def save_results(self, results, output_path):\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Resultados guardados en: {output_path}\")\n",
        "\n",
        "    def process_cv(self, pdf_path):\n",
        "        text = self.extract_text_from_pdf(pdf_path)\n",
        "        if not text:\n",
        "            return {\"error\": \"No se pudo extraer texto del PDF\"}\n",
        "        return self.extract_structured_info(text)\n",
        "\n",
        "# Inicializar modelo de embeddings semánticos\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "def calcular_match_semantico(cv_data, requisitos_texto):\n",
        "    if not requisitos_texto:\n",
        "        return 0.0\n",
        "    cv_texto = \"\\n\".join([cv_data.get(k, \"\") for k in [\"educacion\", \"experiencia\", \"habilidades\", \"idiomas\"]])\n",
        "    emb_cv = model.encode(cv_texto, convert_to_tensor=True)\n",
        "    emb_req = model.encode(requisitos_texto, convert_to_tensor=True)\n",
        "    similarity = util.cos_sim(emb_req, emb_cv).item()\n",
        "    return round(similarity * 100, 2)\n",
        "\n",
        "def definir_puesto_completo():\n",
        "    print(\"\\nIntroduce la descripción completa del puesto:\")\n",
        "    return input(\"> \")\n",
        "\n",
        "def definir_requisitos_estructurados():\n",
        "    requisitos = {}\n",
        "    print(\"\\nIntroduce los requisitos por categoría.\")\n",
        "    requisitos[\"educacion\"] = input(\"Educación requerida: \")\n",
        "    requisitos[\"experiencia\"] = input(\"Experiencia requerida: \")\n",
        "    requisitos[\"habilidades\"] = input(\"Habilidades requeridas: \")\n",
        "    requisitos[\"idiomas\"] = input(\"Idiomas requeridos: \")\n",
        "    return \"\\n\".join([f\"{k}: {v}\" for k, v in requisitos.items()])\n",
        "\n",
        "def menu():\n",
        "    mejores_candidatos = []\n",
        "    extractor = CVInfoExtractor()\n",
        "    output_folder = \"resultados/\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    print(\"=== Sistema de Matching de Candidatos ===\")\n",
        "    print(\"1. Definir un puesto con una descripción completa\")\n",
        "    print(\"2. Definir requisitos separados por categoría\")\n",
        "    opcion = input(\"Selecciona una opción (1/2): \")\n",
        "\n",
        "    if opcion == \"1\":\n",
        "        requisitos_texto = definir_puesto_completo()\n",
        "    elif opcion == \"2\":\n",
        "        requisitos_texto = definir_requisitos_estructurados()\n",
        "    else:\n",
        "        print(\"Opción no válida.\")\n",
        "        return\n",
        "\n",
        "    archivos_cv = [f for f in os.listdir(output_folder) if f.endswith(\".json\")]\n",
        "    if not archivos_cv:\n",
        "        print(\"No se encontraron resultados procesados en la carpeta 'resultados/'.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nResultados de Matching:\\n\") # Corrected indentation of the print statement\n",
        "    ranking = []\n",
        "    for archivo in archivos_cv:\n",
        "        with open(os.path.join(output_folder, archivo), \"r\", encoding=\"utf-8\") as f:\n",
        "            cv_data = json.load(f)\n",
        "            porcentaje = calcular_match_semantico(cv_data, requisitos_texto)\n",
        "            print(f\"{archivo}: {porcentaje}% de coincidencia\") # Corrected indentation\n",
        "        ranking.append((archivo, porcentaje))\n",
        "\n",
        "    ranking.sort(key=lambda x: x[1], reverse=True) # Corrected indentation\n",
        "\n",
        "    print(\"\\nTop candidatos:\") # Corrected indentation\n",
        "    for i, (nombre, score) in enumerate(ranking[:3], 1): # Corrected indentation\n",
        "        print(f\"{i}. {nombre} - {score}%\") # Corrected indentation\n",
        "\n",
        "\n",
        "def procesar_cvs_en_carpeta():\n",
        "    pdf_folder = \"cvs/\"\n",
        "    output_folder = \"resultados/\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    extractor = CVInfoExtractor()\n",
        "\n",
        "    for filename in os.listdir(pdf_folder):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(pdf_folder, filename)\n",
        "            output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_analisis.json\")\n",
        "            print(f\"Procesando: {filename}\")\n",
        "            results = extractor.process_cv(pdf_path)\n",
        "            extractor.save_results(results, output_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    procesar_cvs_en_carpeta()\n",
        "    menu()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "q6Bf59IoAwp3",
        "outputId": "ec67b4b3-a2f0-49bb-9ccf-c37cb6e78fea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando: Curriculum Vitae Rafael Pecino.pdf\n",
            "Resultados guardados en: resultados/Curriculum Vitae Rafael Pecino_analisis.json\n",
            "Procesando: cv-espanol-confoto.pdf\n",
            "Resultados guardados en: resultados/cv-espanol-confoto_analisis.json\n",
            "=== Sistema de Matching de Candidatos ===\n",
            "1. Definir un puesto con una descripción completa\n",
            "2. Definir requisitos separados por categoría\n",
            "Selecciona una opción (1/2): 1\n",
            "\n",
            "Introduce la descripción completa del puesto:\n",
            "> Título del puesto: Técnico de Soporte y Desarrollo Junior  Descripción: Buscamos un profesional con formación en Ingeniería Informática o similar, con experiencia en soporte técnico y desarrollo de aplicaciones, para incorporarse a nuestro equipo IT. Su responsabilidad será participar en el mantenimiento de sistemas, resolución de incidencias de usuarios, automatización de procesos y tareas de desarrollo en proyectos internos.  Responsabilidades:  Brindar soporte técnico a usuarios internos (hardware, software, redes).  Automatizar tareas mediante scripts o herramientas como PowerShell o Python.  Participar en el desarrollo y mantenimiento de aplicaciones web o de escritorio.  Colaborar en proyectos con áreas de infraestructura y desarrollo.  Documentar procedimientos técnicos y crear manuales de usuario.  Requisitos:  Grado en Ingeniería Informática, Ingeniería Técnica o Ciclo Superior en Desarrollo de Aplicaciones.  Conocimientos de bases de datos (SQL), lenguajes de scripting (Python, Bash), y sistemas operativos (Windows/Linux).  Experiencia en entornos de soporte técnico, helpdesk o administración de sistemas.  Nivel medio o superior de inglés (mínimo B1).  Habilidades valoradas:  Conocimientos de desarrollo web (HTML, CSS, JavaScript).  Capacidad de resolución de problemas.  Orientación al usuario.  Trabajo en equipo.  Ubicación: Madrid (presencial o híbrido) Tipo de contrato: Jornada completa, duración determinada con posibilidad de indefinido.\n",
            "\n",
            "Resultados de Matching:\n",
            "\n",
            "cv-espanol-confoto_analisis.json: 59.31% de coincidencia\n",
            "Curriculum Vitae Rafael Pecino_analisis.json: 64.04% de coincidencia\n",
            "\n",
            "Top candidatos:\n",
            "1. Curriculum Vitae Rafael Pecino_analisis.json - 64.04%\n",
            "2. cv-espanol-confoto_analisis.json - 59.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import PyPDF2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class CVInfoExtractor:\n",
        "    def __init__(self):\n",
        "        self.section_patterns = {\n",
        "            \"datos_personales\": [\n",
        "                r\"datos\\s+personales\", r\"contacto\", r\"perfil\\s+personal\",\n",
        "                r\"información\\s+personal\", r\"sobre\\s+mí\", r\"fecha\\s+de\\s+nacimiento\",\n",
        "                r\"dirección\", r\"teléfono\", r\"correo\", r\"email\"\n",
        "            ],\n",
        "            \"educacion\": [\n",
        "                r\"educación\", r\"formación\\s+académica\", r\"datos\\s+académicos\",\n",
        "                r\"estudios\", r\"titulación\", r\"grado\", r\"ciclo\\s+formativo\",\n",
        "                r\"centro\\s+de\\s+formación\", r\"universidad\", r\"bachillerato\",\n",
        "                r\"licenciatura\", r\"master\", r\"doctorado\"\n",
        "            ],\n",
        "            \"experiencia\": [\n",
        "                r\"experiencia\\s+laboral\", r\"experiencia\\s+profesional\",\n",
        "                r\"historial\\s+laboral\", r\"trayectoria\\s+profesional\",\n",
        "                r\"trabajo\", r\"empleos\", r\"prácticas\", r\"formación\\s+en\\s+centros\"\n",
        "            ],\n",
        "            \"habilidades\": [\n",
        "                r\"habilidades\", r\"competencias\", r\"conocimientos\", r\"aptitudes\",\n",
        "                r\"destrezas\", r\"capacidades\", r\"conocimientos\\s+técnicos\",\n",
        "                r\"herramientas\", r\"programas\", r\"actitudes\"\n",
        "            ],\n",
        "            \"idiomas\": [\n",
        "                r\"idiomas\", r\"lenguajes\", r\"nivel\\s+de\\s+idioma\", r\"ingles\", r\"inglés\",\n",
        "                r\"español\", r\"castellano\", r\"nativo\", r\"b1\", r\"b2\", r\"c1\", r\"c2\",\n",
        "                r\"francés\", r\"alemán\", r\"italiano\", r\"portugués\"\n",
        "            ],\n",
        "            \"certificaciones\": [\n",
        "                r\"certificaciones\", r\"certificados\", r\"cursos\", r\"curso\\s+de\",\n",
        "                r\"diplomas\", r\"acreditaciones\", r\"especialización\", r\"diploma\"\n",
        "            ],\n",
        "            \"otros\": [\n",
        "                r\"hobbies\", r\"aficiones\", r\"intereses\", r\"voluntariado\",\n",
        "                r\"referencias\", r\"disponibilidad\", r\"carnet\", r\"permiso\\s+de\\s+conducir\",\n",
        "                r\"carnet\\s+de\\s+conducir\", r\"coche+propio\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                return \" \".join([page.extract_text() or \"\" for page in reader.pages])\n",
        "        except Exception as e:\n",
        "            print(f\"Error al procesar el PDF {pdf_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return re.sub(r'[^\\w\\s\\d\\-\\u00E0-\\u00FC@.,:;+()/%]', '', text)\n",
        "\n",
        "    def split_into_sections(self, text):\n",
        "        section_headers = [\n",
        "            r'\\b(?:DATOS PERSONALES|PERFIL|SOBRE MÍ)\\b',\n",
        "            r'\\b(?:EDUCACIÓN|FORMACIÓN|ESTUDIOS|DATOS ACADÉMICOS)\\b',\n",
        "            r'\\b(?:EXPERIENCIA|HISTORIA LABORAL|EMPLEOS|PRÁCTICAS)\\b',\n",
        "            r'\\b(?:HABILIDADES|COMPETENCIAS|CONOCIMIENTOS|APTITUDES)\\b',\n",
        "            r'\\b(?:IDIOMAS|LENGUAJES)\\b',\n",
        "            r'\\b(?:CERTIFICACIONES|CURSOS|DIPLOMAS)\\b',\n",
        "            r'\\b(?:INFORMACIÓN ADICIONAL|OTROS|AFICIONES|REFERENCIAS)\\b'\n",
        "        ]\n",
        "\n",
        "        section_markers = [(m.start(), m.group()) for p in section_headers for m in re.finditer(p, text, re.IGNORECASE)]\n",
        "        section_markers.sort()\n",
        "\n",
        "        if not section_markers:\n",
        "            return [(text, 0, len(text))]\n",
        "\n",
        "        return [(text[section_markers[i][0]:section_markers[i+1][0] if i+1 < len(section_markers) else len(text)], section_markers[i][0], section_markers[i+1][0] if i+1 < len(section_markers) else len(text)) for i in range(len(section_markers))]\n",
        "\n",
        "    def classify_section(self, section_text):\n",
        "        scores = {}\n",
        "        section_lower = section_text.lower()\n",
        "\n",
        "        for category, patterns in self.section_patterns.items():\n",
        "            scores[category] = sum(len(re.findall(p, section_lower, re.IGNORECASE)) for p in patterns)\n",
        "\n",
        "        best_category = max(scores.items(), key=lambda x: x[1])\n",
        "\n",
        "        if best_category[1] == 0:\n",
        "            if re.search(r'\\d{4}-\\d{4}|\\d{4} - \\d{4}|\\d{2}/\\d{2}/\\d{4}', section_lower):\n",
        "                if re.search(r'universidad|colegio|escuela ', section_lower):\n",
        "                    return \"educacion\", 1\n",
        "                return \"experiencia\", 1\n",
        "            if re.search(r'python|java|c\\+\\+|html|css|javascript', section_lower):\n",
        "                return \"habilidades\", 1\n",
        "            if re.search(r'inglés|español|francés|alemán', section_lower):\n",
        "                return \"idiomas\", 1\n",
        "            return \"otros\", 0\n",
        "\n",
        "        return best_category[0], best_category[1]\n",
        "\n",
        "    def extract_structured_info(self, text):\n",
        "        processed_text = self.preprocess_text(text)\n",
        "        sections = self.split_into_sections(processed_text)\n",
        "\n",
        "        categorized_sections = {}\n",
        "        for section_text, _, _ in sections:\n",
        "            category, _ = self.classify_section(section_text)\n",
        "            categorized_sections.setdefault(category, []).append(section_text)\n",
        "\n",
        "        return {k: \"\\n\\n\".join(v) for k, v in categorized_sections.items()}\n",
        "\n",
        "    def process_cv(self, pdf_path):\n",
        "        text = self.extract_text_from_pdf(pdf_path)\n",
        "        if not text:\n",
        "            return {\"error\": \"No se pudo extraer texto del PDF\"}\n",
        "        return self.extract_structured_info(text)\n",
        "\n",
        "def calcular_match(cv_data, requisitos_texto):\n",
        "    if not requisitos_texto:\n",
        "        return 0.0\n",
        "    cv_texto = \"\\n\".join([v for k, v in cv_data.items() if k in [\"educacion\", \"experiencia\", \"habilidades\", \"idiomas\"]])\n",
        "    corpus = [requisitos_texto, cv_texto]\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "    return round(similarity * 100, 2)\n",
        "\n",
        "def definir_puesto_completo():\n",
        "    print(\"\\nIntroduce la descripción completa del puesto (puedes pegar desde un anuncio):\")\n",
        "    return input(\"Descripción del puesto:\\n> \")\n",
        "\n",
        "def definir_requisitos_estructurados():\n",
        "    requisitos = {}\n",
        "    print(\"\\nIntroduce los requisitos por categoría.\")\n",
        "    requisitos[\"educacion\"] = input(\"Educación requerida: \")\n",
        "    requisitos[\"experiencia\"] = input(\"Experiencia requerida: \")\n",
        "    requisitos[\"habilidades\"] = input(\"Habilidades requeridas: \")\n",
        "    requisitos[\"idiomas\"] = input(\"Idiomas requeridos: \")\n",
        "    return \"\\n\".join([f\"{k}: {v}\" for k, v in requisitos.items()])\n",
        "\n",
        "def menu():\n",
        "    extractor = CVInfoExtractor()\n",
        "    pdf_folder = \"cvs/\"\n",
        "    output_folder = \"resultados/\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    print(\"=== Sistema de Matching de Candidatos ===\")\n",
        "    print(\"1. Definir un puesto con una descripción completa\")\n",
        "    print(\"2. Definir requisitos separados por categoría\")\n",
        "    opcion = input(\"Selecciona una opción (1/2): \")\n",
        "\n",
        "    if opcion == \"1\":\n",
        "        requisitos_texto = definir_puesto_completo()\n",
        "    elif opcion == \"2\":\n",
        "        requisitos_texto = definir_requisitos_estructurados()\n",
        "    else:\n",
        "        print(\"Opción no válida.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(pdf_folder):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(pdf_folder, filename)\n",
        "            output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_analisis.json\")\n",
        "            print(f\"Procesando: {filename}\")\n",
        "            results = extractor.process_cv(pdf_path)\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    archivos_cv = [f for f in os.listdir(output_folder) if f.endswith(\".json\")]\n",
        "    print(\"\\nResultados de Matching:\\n\")\n",
        "    for archivo in archivos_cv:\n",
        "        with open(os.path.join(output_folder, archivo), \"r\", encoding=\"utf-8\") as f:\n",
        "            cv_data = json.load(f)\n",
        "            porcentaje = calcular_match(cv_data, requisitos_texto)\n",
        "            print(f\"{archivo}: {porcentaje}% de coincidencia\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    menu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "RgK5cSb1y67H",
        "outputId": "509855a7-3985-44d6-d3ef-ea91778aaf16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sistema de Matching de Candidatos ===\n",
            "1. Definir un puesto con una descripción completa\n",
            "2. Definir requisitos separados por categoría\n",
            "Selecciona una opción (1/2): 2\n",
            "\n",
            "Introduce los requisitos por categoría.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b82b39e8f81e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mmenu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-b82b39e8f81e>\u001b[0m in \u001b[0;36mmenu\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mrequisitos_texto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefinir_puesto_completo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mopcion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mrequisitos_texto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefinir_requisitos_estructurados\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Opción no válida.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b82b39e8f81e>\u001b[0m in \u001b[0;36mdefinir_requisitos_estructurados\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mrequisitos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nIntroduce los requisitos por categoría.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mrequisitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"educacion\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Educación requerida: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mrequisitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"experiencia\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Experiencia requerida: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mrequisitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"habilidades\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Habilidades requeridas: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "class CVInfoExtractor:\n",
        "    \"\"\"\n",
        "    Extractor de información específica de CVs que separa con precisión\n",
        "    las diferentes secciones usando patrones comunes y contexto\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Inicializa el extractor con patrones para reconocer secciones\"\"\"\n",
        "\n",
        "        # Patrones para identificar secciones\n",
        "        self.section_patterns = {\n",
        "            \"datos_personales\": [\n",
        "                r\"datos\\s+personales\", r\"contacto\", r\"perfil\\s+personal\",\n",
        "                r\"información\\s+personal\", r\"sobre\\s+mí\", r\"fecha\\s+de\\s+nacimiento\",\n",
        "                r\"dirección\", r\"teléfono\", r\"correo\", r\"email\"\n",
        "            ],\n",
        "            \"educacion\": [\n",
        "                r\"educación\", r\"formación\\s+académica\", r\"datos\\s+académicos\",\n",
        "                r\"estudios\", r\"titulación\", r\"grado\", r\"ciclo\\s+formativo\",\n",
        "                r\"centro\\s+de\\s+formación\", r\"universidad\", r\"bachillerato\",\n",
        "                r\"licenciatura\", r\"master\", r\"doctorado\"\n",
        "            ],\n",
        "            \"experiencia\": [\n",
        "                r\"experiencia\\s+laboral\", r\"experiencia\\s+profesional\",\n",
        "                r\"historial\\s+laboral\", r\"trayectoria\\s+profesional\",\n",
        "                r\"trabajo\", r\"empleos\", r\"prácticas\", r\"formación\\s+en\\s+centros\"\n",
        "                r\"experiencia\\s+laboral\", r\"experiencia\\s+profesional\",\n",
        "                r\"historial\\s+laboral\", r\"trayectoria\\s+profesional\",\n",
        "                r\"trabajo\", r\"empleos\", r\"prácticas\", r\"formación\\s+en\\s+centros\"\n",
        "            ],\n",
        "            \"habilidades\": [\n",
        "                r\"habilidades\", r\"competencias\", r\"conocimientos\", r\"aptitudes\",\n",
        "                r\"destrezas\", r\"capacidades\", r\"conocimientos\\s+técnicos\",\n",
        "                r\"herramientas\", r\"programas\", r\"actitudes\"\n",
        "            ],\n",
        "            \"idiomas\": [\n",
        "                r\"idiomas\", r\"lenguajes\", r\"nivel\\s+de\\s+idioma\", r\"ingles\", r\"inglés\",\n",
        "                r\"español\", r\"castellano\", r\"nativo\", r\"b1\", r\"b2\", r\"c1\", r\"c2\"\n",
        "                r\"francés\", r\"alemán\", r\"italiano\", r\"portugués\"\n",
        "            ],\n",
        "            \"certificaciones\": [\n",
        "                r\"certificaciones\", r\"certificados\", r\"cursos\", r\"curso\\s+de\",\n",
        "                r\"diplomas\", r\"acreditaciones\", r\"especialización\", r\"diploma\"\n",
        "\n",
        "            ],\n",
        "            \"otros\": [\n",
        "                r\"hobbies\", r\"aficiones\", r\"intereses\", r\"voluntariado\",\n",
        "                r\"referencias\", r\"disponibilidad\", r\"carnet\", r\"permiso\\s+de\\s+conducir\", r\"carnet\\s+de\\s+conducir\",\n",
        "                r\"coche+propio\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        \"\"\"\n",
        "        Extrae texto de un archivo PDF\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Ruta al archivo PDF\n",
        "\n",
        "        Returns:\n",
        "            str: Texto extraído del PDF\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text()\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"Error al procesar el PDF {pdf_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocesa el texto para una mejor extracción\n",
        "\n",
        "        Args:\n",
        "            text: Texto del CV\n",
        "\n",
        "        Returns:\n",
        "            str: Texto preprocesado\n",
        "        \"\"\"\n",
        "        # Reemplazar múltiples espacios por uno solo\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Eliminar caracteres especiales pero mantener acentos\n",
        "        text = re.sub(r'[^\\w\\s\\d\\-áéíóúÁÉÍÓÚñÑüÜ@.,:;+()/%]', '', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def split_into_sections(self, text):\n",
        "        \"\"\"\n",
        "        Divide el texto en secciones basadas en separadores comunes\n",
        "\n",
        "        Args:\n",
        "            text: Texto del CV\n",
        "\n",
        "        Returns:\n",
        "            list: Lista de secciones [(texto, índice_inicio, índice_fin)]\n",
        "        \"\"\"\n",
        "        # Buscar patrones que indican el inicio de una sección\n",
        "        section_markers = []\n",
        "\n",
        "        # Patrones comunes de secciones en CVs\n",
        "        section_headers = [\n",
        "            r'\\b(?:DATOS PERSONALES|PERFIL|SOBRE MÍ)\\b',\n",
        "            r'\\b(?:EDUCACIÓN|FORMACIÓN|ESTUDIOS|DATOS ACADÉMICOS)\\b',\n",
        "            r'\\b(?:EXPERIENCIA|HISTORIA LABORAL|EMPLEOS|PRÁCTICAS)\\b',\n",
        "            r'\\b(?:HABILIDADES|COMPETENCIAS|CONOCIMIENTOS|APTITUDES)\\b',\n",
        "            r'\\b(?:IDIOMAS|LENGUAJES)\\b',\n",
        "            r'\\b(?:CERTIFICACIONES|CURSOS|DIPLOMAS)\\b',\n",
        "            r'\\b(?:INFORMACIÓN ADICIONAL|OTROS|AFICIONES|REFERENCIAS)\\b'\n",
        "        ]\n",
        "\n",
        "        # Encontrar todas las posibles secciones\n",
        "        for pattern in section_headers:\n",
        "            for match in re.finditer(pattern, text, re.IGNORECASE):\n",
        "                section_markers.append((match.start(), match.group()))\n",
        "\n",
        "        # Ordenar por posición\n",
        "        section_markers.sort()\n",
        "\n",
        "        # Dividir el texto en secciones\n",
        "        sections = []\n",
        "        for i in range(len(section_markers)):\n",
        "            start = section_markers[i][0]\n",
        "            end = section_markers[i+1][0] if i < len(section_markers) - 1 else len(text)\n",
        "            section_text = text[start:end].strip()\n",
        "            sections.append((section_text, start, end))\n",
        "\n",
        "        # Si no se encontraron secciones, tratar todo el texto como una sección\n",
        "        if not sections:\n",
        "            sections.append((text, 0, len(text)))\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def classify_section(self, section_text):\n",
        "        \"\"\"\n",
        "        Clasifica una sección según los patrones de coincidencia\n",
        "\n",
        "        Args:\n",
        "            section_text: Texto de la sección\n",
        "\n",
        "        Returns:\n",
        "            tuple: (categoría, puntuación)\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "        section_lower = section_text.lower()\n",
        "\n",
        "        for category, patterns in self.section_patterns.items():\n",
        "            score = 0\n",
        "            for pattern in patterns:\n",
        "                matches = re.findall(pattern, section_lower, re.IGNORECASE)\n",
        "                score += len(matches)\n",
        "            scores[category] = score\n",
        "\n",
        "        # Determinar la categoría con mayor puntuación\n",
        "        best_category = max(scores.items(), key=lambda x: x[1])\n",
        "\n",
        "        # Si no hay coincidencias claras, analizar el contexto\n",
        "        if best_category[1] == 0:\n",
        "            # Buscar pistas contextuales\n",
        "            if re.search(r'\\d{4}-\\d{4}|\\d{4} - \\d{4}|\\d{2}/\\d{2}/\\d{4}', section_lower):\n",
        "                if re.search(r'universidad|colegio|escuela ', section_lower):\n",
        "                    return \"educacion\", 1\n",
        "                return \"experiencia\", 1\n",
        "\n",
        "            if re.search(r'python|java|c\\+\\+|html|css|javascript', section_lower):\n",
        "                return \"habilidades\", 1\n",
        "\n",
        "            if re.search(r'inglés|español|francés|alemán', section_lower):\n",
        "                return \"idiomas\", 1\n",
        "\n",
        "            # Default para secciones sin coincidencias claras\n",
        "            return \"otros\", 0\n",
        "\n",
        "        return best_category[0], best_category[1]\n",
        "\n",
        "    def extract_structured_info(self, text):\n",
        "        \"\"\"\n",
        "        Extrae y estructura la información del CV en categorías\n",
        "\n",
        "        Args:\n",
        "            text: Texto completo del CV\n",
        "\n",
        "        Returns:\n",
        "            dict: Información estructurada por categorías\n",
        "        \"\"\"\n",
        "        # Preprocesar el texto\n",
        "        processed_text = self.preprocess_text(text)\n",
        "\n",
        "        # Dividir en secciones\n",
        "        sections = self.split_into_sections(processed_text)\n",
        "\n",
        "        # Clasificar cada sección\n",
        "        categorized_sections = {}\n",
        "        for section_text, _, _ in sections:\n",
        "            category, score = self.classify_section(section_text)\n",
        "\n",
        "            if category not in categorized_sections:\n",
        "                categorized_sections[category] = []\n",
        "\n",
        "            categorized_sections[category].append(section_text)\n",
        "\n",
        "        # Si alguna categoría principal está vacía, intentar extraerla del contexto general\n",
        "        main_categories = [\"datos_personales\", \"educacion\", \"experiencia\", \"habilidades\", \"idiomas\"]\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for category in main_categories:\n",
        "            if category not in categorized_sections:\n",
        "                extracted = self.extract_specific_info(text_lower, category)\n",
        "                if extracted:\n",
        "                    categorized_sections[category] = [extracted]\n",
        "\n",
        "        # Unificar el texto por categoría\n",
        "        result = {}\n",
        "        for category, sections in categorized_sections.items():\n",
        "            result[category] = \"\\n\\n\".join(sections)\n",
        "\n",
        "        # Asegurar que todas las categorías estén presentes\n",
        "        for category in self.section_patterns.keys():\n",
        "            if category not in result:\n",
        "                result[category] = \"No se encontró información\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def extract_specific_info(self, text, category):\n",
        "        \"\"\"\n",
        "        Extrae información específica basada en la categoría\n",
        "\n",
        "        Args:\n",
        "            text: Texto completo del CV\n",
        "            category: Categoría a extraer\n",
        "\n",
        "        Returns:\n",
        "            str: Información extraída\n",
        "        \"\"\"\n",
        "        result = []\n",
        "\n",
        "        if category == \"datos_personales\":\n",
        "            # Extraer correo electrónico\n",
        "            email = re.search(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', text)\n",
        "            if email:\n",
        "                result.append(f\"Email: {email.group()}\")\n",
        "\n",
        "            # Extraer teléfono\n",
        "            phone = re.search(r'(?:\\+\\d{1,3}[-\\s]?)?\\(?(?:\\d{3})?\\)?[-\\s]?\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{2}', text)\n",
        "            if phone:\n",
        "                result.append(f\"Teléfono: {phone.group()}\")\n",
        "\n",
        "            # Extraer fecha de nacimiento\n",
        "            birth = re.search(r'\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}', text)\n",
        "            if birth:\n",
        "                result.append(f\"Fecha de nacimiento: {birth.group()}\")\n",
        "\n",
        "        elif category == \"educacion\":\n",
        "            # Buscar patrones de educación\n",
        "            education = re.findall(r'(?:grado|ciclo|formación|curso|universidad|superior).*?(?:\\d{4}|\\d{2}-\\d{2})', text, re.IGNORECASE)\n",
        "            result.extend(education)\n",
        "\n",
        "        elif category == \"experiencia\":\n",
        "            # Buscar patrones de experiencia laboral\n",
        "            experience = re.findall(r'(?:experiencia|trabajo|empresa|laboral).*?(?:\\d{4}|\\d{2}-\\d{2})', text, re.IGNORECASE)\n",
        "            result.extend(experience)\n",
        "\n",
        "        elif category == \"habilidades\":\n",
        "            # Extraer habilidades (separadas por comas o guiones)\n",
        "            skills = re.findall(r'(?:conocimientos|habilidades|competencias):?\\s+(.*?)(?:\\.|$)', text, re.IGNORECASE)\n",
        "            result.extend(skills)\n",
        "\n",
        "            # Buscar lenguajes de programación comunes\n",
        "            prog_langs = re.findall(r'(?:python|java|javascript|html|css|c\\+\\+|c#|ruby|php|sql|react|angular)[a-z]*', text, re.IGNORECASE)\n",
        "            if prog_langs:\n",
        "                result.append(\"Lenguajes de programación: \" + \", \".join(set(prog_langs)))\n",
        "\n",
        "        elif category == \"idiomas\":\n",
        "            # Extraer menciones a idiomas\n",
        "            languages = re.findall(r'(?:inglés|español|francés|alemán|italiano|portugués)(?::\\s*|\\s+)(nativo|[abc][1-2]|básico|intermedio|avanzado|fluido)?', text, re.IGNORECASE)\n",
        "            for lang in languages:\n",
        "                if isinstance(lang, tuple):\n",
        "                    result.append(f\"{lang[0]}: {lang[1] if lang[1] else 'mencionado'}\")\n",
        "                else:\n",
        "                    result.append(lang)\n",
        "\n",
        "        return \"\\n\".join(result) if result else \"\"\n",
        "\n",
        "    def process_cv(self, pdf_path):\n",
        "        \"\"\"\n",
        "        Procesa un CV completo\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Ruta al archivo PDF\n",
        "\n",
        "        Returns:\n",
        "            dict: Información estructurada\n",
        "        \"\"\"\n",
        "        # Extraer texto del PDF\n",
        "        text = self.extract_text_from_pdf(pdf_path)\n",
        "        if not text:\n",
        "            return {\"error\": \"No se pudo extraer texto del PDF\"}\n",
        "\n",
        "        # Extraer información estructurada\n",
        "        return self.extract_structured_info(text)\n",
        "\n",
        "    def process_text(self, text):\n",
        "        \"\"\"\n",
        "        Procesa un texto de CV directamente\n",
        "\n",
        "        Args:\n",
        "            text: Texto del CV\n",
        "\n",
        "        Returns:\n",
        "            dict: Información estructurada\n",
        "        \"\"\"\n",
        "        return self.extract_structured_info(text)\n",
        "\n",
        "    def save_results(self, results, output_path):\n",
        "        \"\"\"\n",
        "        Guarda los resultados en formato JSON\n",
        "\n",
        "        Args:\n",
        "            results: Resultados de la extracción\n",
        "            output_path: Ruta para guardar el archivo\n",
        "        \"\"\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Resultados guardados en: {output_path}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Función principal\"\"\"\n",
        "    pdf_folder = \"cvs/\"\n",
        "    output_folder = \"resultados/\"\n",
        "\n",
        "    # Crear carpeta de salida si no existe\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Inicializar extractor\n",
        "    extractor = CVInfoExtractor()\n",
        "\n",
        "    # Procesar todos los PDFs en la carpeta\n",
        "    for filename in os.listdir(pdf_folder):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(pdf_folder, filename)\n",
        "            output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_analisis.json\")\n",
        "\n",
        "            print(f\"Procesando: {filename}\")\n",
        "            results = extractor.process_cv(pdf_path)\n",
        "            extractor.save_results(results, output_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mngqkJFUTObD",
        "outputId": "9a9998ae-f15f-455b-ddc8-6881153edc11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando: Curriculum Vitae Rafael Pecino.pdf\n",
            "Resultados guardados en: resultados/Curriculum Vitae Rafael Pecino_analisis.json\n",
            "Procesando: cv-espanol-confoto.pdf\n",
            "Resultados guardados en: resultados/cv-espanol-confoto_analisis.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLASIFICACION DE CVS-RESULTADOS EN JSON\n"
      ],
      "metadata": {
        "id": "CVhfKXZvwk9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "class JobMatcher:\n",
        "    \"\"\"\n",
        "    Sistema para encontrar coincidencias entre CVs y descripciones de puesto de trabajo,\n",
        "    calculando un porcentaje de match basado en las habilidades requeridas, experiencia\n",
        "    y otros factores relevantes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Inicializa el sistema de matching\"\"\"\n",
        "        # Inicializar el extractor de CVs\n",
        "        self.cv_extractor = CVInfoExtractor()\n",
        "\n",
        "        # Descargar recursos de NLTK si es necesario (solo la primera vez)\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "            nltk.data.find('corpora/stopwords')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "\n",
        "        # Definir stop words en español\n",
        "        self.stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "        # Definir pesos para cada categoría en el proceso de matching\n",
        "        self.category_weights = {\n",
        "            \"experiencia\": 0.35,\n",
        "            \"habilidades\": 0.30,\n",
        "            \"educacion\": 0.15,\n",
        "            \"idiomas\": 0.15,\n",
        "            \"certificaciones\": 0.05\n",
        "        }\n",
        "\n",
        "        # Definir vectorizador TF-IDF\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            min_df=1,\n",
        "            stop_words=self.stopwords,\n",
        "            lowercase=True,\n",
        "            analyzer='word',\n",
        "            ngram_range=(1, 2)  # Unigrams y bigrams\n",
        "        )\n",
        "\n",
        "    def parse_job_description(self, job_file):\n",
        "        \"\"\"\n",
        "        Parsea un archivo JSON de descripción de trabajo\n",
        "\n",
        "        Args:\n",
        "            job_file: Ruta al archivo JSON con la descripción del puesto\n",
        "\n",
        "        Returns:\n",
        "            dict: Información estructurada del puesto\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(job_file, 'r', encoding='utf-8') as f:\n",
        "                job_data = json.load(f)\n",
        "\n",
        "            # Verificar que tenga la estructura mínima necesaria\n",
        "            required_fields = [\"titulo\", \"descripcion\", \"requisitos\"]\n",
        "\n",
        "            for field in required_fields:\n",
        "                if field not in job_data:\n",
        "                    raise ValueError(f\"El archivo de trabajo debe contener el campo '{field}'\")\n",
        "\n",
        "            # Estructurar descripción del trabajo en las mismas categorías que los CVs\n",
        "            structured_job = {\n",
        "                \"titulo\": job_data[\"titulo\"],\n",
        "                \"experiencia\": self._extract_job_experience(job_data),\n",
        "                \"habilidades\": self._extract_job_skills(job_data),\n",
        "                \"educacion\": self._extract_job_education(job_data),\n",
        "                \"idiomas\": self._extract_job_languages(job_data),\n",
        "                \"certificaciones\": self._extract_job_certifications(job_data)\n",
        "            }\n",
        "\n",
        "            return structured_job\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al procesar la descripción del trabajo: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_job_experience(self, job_data):\n",
        "        \"\"\"Extrae información sobre experiencia requerida\"\"\"\n",
        "        experience_text = \"\"\n",
        "\n",
        "        # Buscar en requisitos\n",
        "        if \"requisitos\" in job_data:\n",
        "            for req in job_data[\"requisitos\"]:\n",
        "                if any(re.search(r'experi[e|é]ncia|años|laborales', req, re.IGNORECASE)):\n",
        "                    experience_text += req + \" \"\n",
        "\n",
        "        # Buscar en descripción general\n",
        "        if \"descripcion\" in job_data:\n",
        "            exp_matches = re.findall(\n",
        "                r'(?:se requiere|necesitamos|buscamos|experiencia)(?:.{0,30})(?:años|experiencia)(?:.{0,50})',\n",
        "                job_data[\"descripcion\"],\n",
        "                re.IGNORECASE\n",
        "            )\n",
        "            experience_text += \" \".join(exp_matches)\n",
        "\n",
        "        return experience_text.strip()\n",
        "\n",
        "    def _extract_job_skills(self, job_data):\n",
        "        \"\"\"Extrae habilidades requeridas para el puesto\"\"\"\n",
        "        skills_text = \"\"\n",
        "\n",
        "        # Extraer de requisitos técnicos o habilidades específicas\n",
        "        if \"requisitos\" in job_data:\n",
        "            for req in job_data[\"requisitos\"]:\n",
        "                if not any(re.search(r'experi[e|é]ncia|años|titulación|idiomas|certificado', req, re.IGNORECASE)):\n",
        "                    skills_text += req + \" \"\n",
        "\n",
        "        # Buscar sección específica de skills o competencias\n",
        "        if \"habilidades\" in job_data:\n",
        "            if isinstance(job_data[\"habilidades\"], list):\n",
        "                skills_text += \" \".join(job_data[\"habilidades\"])\n",
        "            else:\n",
        "                skills_text += job_data[\"habilidades\"]\n",
        "\n",
        "        # Buscar habilidades específicas en la descripción\n",
        "        skill_patterns = [\n",
        "            r'conocimientos (?:en|de) ([\\w\\s,]+)',\n",
        "            r'habilidades (?:en|de) ([\\w\\s,]+)',\n",
        "            r'dominio (?:en|de) ([\\w\\s,]+)',\n",
        "            r'manejo (?:en|de) ([\\w\\s,]+)'\n",
        "        ]\n",
        "\n",
        "        if \"descripcion\" in job_data:\n",
        "            for pattern in skill_patterns:\n",
        "                matches = re.findall(pattern, job_data[\"descripcion\"], re.IGNORECASE)\n",
        "                skills_text += \" \".join(matches)\n",
        "\n",
        "        return skills_text.strip()\n",
        "\n",
        "    def _extract_job_education(self, job_data):\n",
        "        \"\"\"Extrae requisitos educativos\"\"\"\n",
        "        education_text = \"\"\n",
        "\n",
        "        # Buscar en requisitos\n",
        "        if \"requisitos\" in job_data:\n",
        "            for req in job_data[\"requisitos\"]:\n",
        "                if any(re.search(r'grado|licenciatura|titulación|formación|estudios|universidad', req, re.IGNORECASE)):\n",
        "                    education_text += req + \" \"\n",
        "\n",
        "        # Buscar en formación académica\n",
        "        if \"formacion\" in job_data:\n",
        "            if isinstance(job_data[\"formacion\"], list):\n",
        "                education_text += \" \".join(job_data[\"formacion\"])\n",
        "            else:\n",
        "                education_text += job_data[\"formacion\"]\n",
        "\n",
        "        return education_text.strip()\n",
        "\n",
        "    def _extract_job_languages(self, job_data):\n",
        "        \"\"\"Extrae requisitos de idiomas\"\"\"\n",
        "        languages_text = \"\"\n",
        "\n",
        "        # Buscar en requisitos\n",
        "        if \"requisitos\" in job_data:\n",
        "            for req in job_data[\"requisitos\"]:\n",
        "                if any(re.search(r'idiomas?|inglés|francés|alemán|italiano|español|nivel', req, re.IGNORECASE)):\n",
        "                    languages_text += req + \" \"\n",
        "\n",
        "        # Buscar en sección específica de idiomas\n",
        "        if \"idiomas\" in job_data:\n",
        "            if isinstance(job_data[\"idiomas\"], list):\n",
        "                languages_text += \" \".join(job_data[\"idiomas\"])\n",
        "            else:\n",
        "                languages_text += job_data[\"idiomas\"]\n",
        "\n",
        "        return languages_text.strip()\n",
        "\n",
        "    def _extract_job_certifications(self, job_data):\n",
        "        \"\"\"Extrae certificaciones requeridas\"\"\"\n",
        "        certifications_text = \"\"\n",
        "\n",
        "        # Buscar en requisitos\n",
        "        if \"requisitos\" in job_data:\n",
        "            for req in job_data[\"requisitos\"]:\n",
        "                if any(re.search(r'certificado|certificación|título|acreditación|carnet', req, re.IGNORECASE)):\n",
        "                    certifications_text += req + \" \"\n",
        "\n",
        "        # Buscar en descripción general\n",
        "        if \"descripcion\" in job_data:\n",
        "            cert_matches = re.findall(\n",
        "                r'(?:certificado|certificación|acreditación)(?:\\s+en|\\s+de)?\\s+([\\w\\s]+)',\n",
        "                job_data[\"descripcion\"],\n",
        "                re.IGNORECASE\n",
        "            )\n",
        "            certifications_text += \" \".join(cert_matches)\n",
        "\n",
        "        return certifications_text.strip()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocesa el texto para el análisis\n",
        "\n",
        "        Args:\n",
        "            text: Texto a preprocesar\n",
        "\n",
        "        Returns:\n",
        "            str: Texto preprocesado\n",
        "        \"\"\"\n",
        "        # Convertir a minúsculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # Tokenizar\n",
        "        tokens = word_tokenize(text, language='spanish')\n",
        "\n",
        "        # Eliminar stopwords y palabras muy cortas\n",
        "        tokens = [t for t in tokens if t not in self.stopwords and len(t) > 2]\n",
        "\n",
        "        # Volver a unir en texto\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def calculate_similarity(self, cv_text, job_text):\n",
        "        \"\"\"\n",
        "        Calcula la similitud entre textos usando TF-IDF y similitud del coseno\n",
        "\n",
        "        Args:\n",
        "            cv_text: Texto del currículum\n",
        "            job_text: Texto de la descripción del trabajo\n",
        "\n",
        "        Returns:\n",
        "            float: Valor de similitud entre 0 y 1\n",
        "        \"\"\"\n",
        "        if not cv_text or not job_text:\n",
        "            return 0.0\n",
        "\n",
        "        # Preprocesar textos\n",
        "        cv_processed = self.preprocess_text(cv_text)\n",
        "        job_processed = self.preprocess_text(job_text)\n",
        "\n",
        "        if not cv_processed or not job_processed:\n",
        "            return 0.0\n",
        "\n",
        "        # Vectorizar textos\n",
        "        try:\n",
        "            tfidf_matrix = self.vectorizer.fit_transform([cv_processed, job_processed])\n",
        "\n",
        "            # Calcular similitud del coseno\n",
        "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "            return max(0.0, min(similarity, 1.0))  # Asegurar que está entre 0 y 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error al calcular similitud: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_match(self, cv_data, job_data):\n",
        "        \"\"\"\n",
        "        Calcula el porcentaje de match entre un CV y un puesto de trabajo\n",
        "\n",
        "        Args:\n",
        "            cv_data: Datos estructurados del CV\n",
        "            job_data: Datos estructurados del puesto\n",
        "\n",
        "        Returns:\n",
        "            dict: Resultados del match con porcentajes\n",
        "        \"\"\"\n",
        "        match_scores = {}\n",
        "        total_weight_used = 0\n",
        "\n",
        "        # Calcular match por categoría\n",
        "        for category, weight in self.category_weights.items():\n",
        "            if category in cv_data and category in job_data:\n",
        "                cv_text = cv_data[category]\n",
        "                job_text = job_data[category]\n",
        "\n",
        "                # Si ambos textos existen, calcular similitud\n",
        "                if cv_text and job_text and cv_text != \"No se encontró información\":\n",
        "                    similarity = self.calculate_similarity(cv_text, job_text)\n",
        "                    match_scores[category] = similarity\n",
        "                    total_weight_used += weight\n",
        "                else:\n",
        "                    match_scores[category] = 0.0\n",
        "            else:\n",
        "                match_scores[category] = 0.0\n",
        "\n",
        "        # Calcular puntaje global ponderado\n",
        "        weighted_score = sum(\n",
        "            match_scores[cat] * self.category_weights[cat]\n",
        "            for cat in self.category_weights.keys()\n",
        "        )\n",
        "\n",
        "        # Normalizar si no se usaron todas las categorías\n",
        "        if total_weight_used > 0 and total_weight_used < 1.0:\n",
        "            weighted_score /= total_weight_used\n",
        "\n",
        "        # Crear resultado detallado\n",
        "        result = {\n",
        "            \"match_porcentaje\": round(weighted_score * 100, 2),\n",
        "            \"match_por_categoria\": {\n",
        "                cat: round(score * 100, 2)\n",
        "                for cat, score in match_scores.items()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def find_keyword_matches(self, cv_data, job_data):\n",
        "        \"\"\"\n",
        "        Encuentra coincidencias de palabras clave entre el CV y el puesto\n",
        "\n",
        "        Args:\n",
        "            cv_data: Datos estructurados del CV\n",
        "            job_data: Datos estructurados del puesto\n",
        "\n",
        "        Returns:\n",
        "            dict: Palabras clave coincidentes\n",
        "        \"\"\"\n",
        "        # Extraer palabras clave de la descripción del trabajo\n",
        "        job_keywords = {}\n",
        "\n",
        "        for category in [\"habilidades\", \"experiencia\"]:\n",
        "            if category in job_data and job_data[category]:\n",
        "                # Extraer posibles keywords\n",
        "                text = job_data[category].lower()\n",
        "\n",
        "                # Palabras clave técnicas (ej. lenguajes de programación, herramientas)\n",
        "                if category == \"habilidades\":\n",
        "                    tech_keywords = re.findall(\n",
        "                        r'\\b(?:python|java|javascript|html|css|angular|react|node|sql|aws|azure|docker|kubernetes|excel|powerbi|tableau|sap|jira|scrum|kanban|agile)\\b',\n",
        "                        text,\n",
        "                        re.IGNORECASE\n",
        "                    )\n",
        "                    job_keywords[\"tecnologias\"] = list(set([kw.lower() for kw in tech_keywords]))\n",
        "\n",
        "                # Experiencia (ej. años, sectores)\n",
        "                if category == \"experiencia\":\n",
        "                    # Años de experiencia\n",
        "                    years = re.findall(r'(\\d+)(?:\\+)?\\s*(?:años|year)', text)\n",
        "                    if years:\n",
        "                        job_keywords[\"años_experiencia\"] = max([int(y) for y in years])\n",
        "\n",
        "                    # Sectores o áreas\n",
        "                    sectors = re.findall(\n",
        "                        r'(?:sector|industria|área)\\s+(?:de|en)?\\s+([\\w\\s]+?)(?:\\.|\\,|;|$)',\n",
        "                        text\n",
        "                    )\n",
        "                    job_keywords[\"sectores\"] = list(set([s.strip().lower() for s in sectors]))\n",
        "\n",
        "        # Buscar coincidencias en el CV\n",
        "        matches = {\n",
        "            \"tecnologias\": [],\n",
        "            \"años_experiencia\": {\"requerido\": job_keywords.get(\"años_experiencia\", 0), \"encontrado\": 0},\n",
        "            \"sectores\": []\n",
        "        }\n",
        "\n",
        "        # Verificar tecnologías\n",
        "        if \"tecnologias\" in job_keywords and \"habilidades\" in cv_data:\n",
        "            cv_text = cv_data[\"habilidades\"].lower()\n",
        "            for tech in job_keywords[\"tecnologias\"]:\n",
        "                if re.search(r'\\b' + re.escape(tech) + r'\\b', cv_text, re.IGNORECASE):\n",
        "                    matches[\"tecnologias\"].append(tech)\n",
        "\n",
        "        # Verificar años de experiencia\n",
        "        if \"años_experiencia\" in job_keywords and \"experiencia\" in cv_data:\n",
        "            cv_text = cv_data[\"experiencia\"].lower()\n",
        "            years_found = re.findall(r'(\\d+)(?:\\+)?\\s*(?:años|year)', cv_text)\n",
        "            if years_found:\n",
        "                matches[\"años_experiencia\"][\"encontrado\"] = max([int(y) for y in years_found])\n",
        "\n",
        "        # Verificar sectores\n",
        "        if \"sectores\" in job_keywords and \"experiencia\" in cv_data:\n",
        "            cv_text = cv_data[\"experiencia\"].lower()\n",
        "            for sector in job_keywords.get(\"sectores\", []):\n",
        "                if re.search(r'\\b' + re.escape(sector) + r'\\b', cv_text, re.IGNORECASE):\n",
        "                    matches[\"sectores\"].append(sector)\n",
        "\n",
        "        return matches\n",
        "\n",
        "    def generate_improvement_suggestions(self, cv_data, job_data, match_result):\n",
        "        \"\"\"\n",
        "        Genera sugerencias para mejorar el CV basado en el match\n",
        "\n",
        "        Args:\n",
        "            cv_data: Datos estructurados del CV\n",
        "            job_data: Datos estructurados del puesto\n",
        "            match_result: Resultados del cálculo de match\n",
        "\n",
        "        Returns:\n",
        "            list: Sugerencias de mejora\n",
        "        \"\"\"\n",
        "        suggestions = []\n",
        "\n",
        "        # Categorías con bajo match\n",
        "        low_match_categories = [\n",
        "            cat for cat, score in match_result[\"match_por_categoria\"].items()\n",
        "            if score < 50 and self.category_weights[cat] >= 0.15  # Solo categorías importantes\n",
        "        ]\n",
        "\n",
        "        # Analizar categorías con bajo match\n",
        "        for category in low_match_categories:\n",
        "            if category == \"habilidades\":\n",
        "                # Extraer habilidades mencionadas en la oferta pero no en el CV\n",
        "                job_skills = set(self._extract_keywords(job_data[\"habilidades\"], n=15))\n",
        "                cv_skills = set(self._extract_keywords(cv_data[\"habilidades\"], n=15))\n",
        "\n",
        "                missing_skills = job_skills - cv_skills\n",
        "                if missing_skills:\n",
        "                    skills_str = \", \".join(list(missing_skills)[:5])  # Mostrar hasta 5\n",
        "                    suggestions.append(\n",
        "                        f\"Añade habilidades relevantes como {skills_str} que se mencionan en la oferta.\"\n",
        "                    )\n",
        "\n",
        "            elif category == \"experiencia\":\n",
        "                # Verificar años de experiencia\n",
        "                job_years = re.search(r'(\\d+)(?:\\+)?\\s*(?:años|year)', job_data[\"experiencia\"])\n",
        "                cv_years = re.search(r'(\\d+)(?:\\+)?\\s*(?:años|year)', cv_data[\"experiencia\"])\n",
        "\n",
        "                if job_years and not cv_years:\n",
        "                    suggestions.append(\n",
        "                        f\"Destaca claramente tus años de experiencia, la oferta menciona {job_years.group(0)}.\"\n",
        "                    )\n",
        "\n",
        "                # Buscar sectores o áreas específicas\n",
        "                job_sectors = self._extract_keywords(job_data[\"experiencia\"], n=5)\n",
        "                for sector in job_sectors:\n",
        "                    if sector not in cv_data[\"experiencia\"].lower():\n",
        "                        suggestions.append(\n",
        "                            f\"Menciona tu experiencia en '{sector}' si la tienes.\"\n",
        "                        )\n",
        "\n",
        "            elif category == \"idiomas\":\n",
        "                if job_data[\"idiomas\"] and (\n",
        "                    not cv_data[\"idiomas\"] or cv_data[\"idiomas\"] == \"No se encontró información\"\n",
        "                ):\n",
        "                    suggestions.append(\n",
        "                        \"Incluye una sección de idiomas en tu CV, ya que se requieren para este puesto.\"\n",
        "                    )\n",
        "                elif \"inglés\" in job_data[\"idiomas\"].lower() and \"inglés\" not in cv_data[\"idiomas\"].lower():\n",
        "                    suggestions.append(\n",
        "                        \"Menciona tu nivel de inglés, ya que se requiere para este puesto.\"\n",
        "                    )\n",
        "\n",
        "            elif category == \"educacion\":\n",
        "                # Verificar requisitos de educación\n",
        "                job_edu_keywords = [\"grado\", \"licenciatura\", \"máster\", \"doctorado\", \"universitario\"]\n",
        "                job_edu_level = None\n",
        "\n",
        "                for kw in job_edu_keywords:\n",
        "                    if kw in job_data[\"educacion\"].lower():\n",
        "                        job_edu_level = kw\n",
        "                        break\n",
        "\n",
        "                if job_edu_level and job_edu_level not in cv_data[\"educacion\"].lower():\n",
        "                    suggestions.append(\n",
        "                        f\"El puesto requiere nivel de {job_edu_level}. Asegúrate de destacar tu formación académica.\"\n",
        "                    )\n",
        "\n",
        "        # Sugerencias generales basadas en el match global\n",
        "        if match_result[\"match_porcentaje\"] < 40:\n",
        "            suggestions.append(\n",
        "                \"Tu CV y este puesto tienen pocas coincidencias. Considera adaptar más tu CV a los requisitos específicos.\"\n",
        "            )\n",
        "        elif match_result[\"match_porcentaje\"] < 60:\n",
        "            suggestions.append(\n",
        "                \"Adapta la terminología de tu CV para que coincida mejor con la usada en la descripción del puesto.\"\n",
        "            )\n",
        "\n",
        "        return suggestions\n",
        "\n",
        "    def _extract_keywords(self, text, n=10):\n",
        "        \"\"\"\n",
        "        Extrae las palabras clave más relevantes de un texto\n",
        "\n",
        "        Args:\n",
        "            text: Texto a analizar\n",
        "            n: Número de palabras clave a extraer\n",
        "\n",
        "        Returns:\n",
        "            list: Lista de palabras clave\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Preprocesar texto\n",
        "        processed = self.preprocess_text(text)\n",
        "\n",
        "        # Usar vectorizador TF-IDF para extraer términos importantes\n",
        "        try:\n",
        "            tfidf = TfidfVectorizer(\n",
        "                max_features=n,\n",
        "                stop_words=self.stopwords,\n",
        "                ngram_range=(1, 2)\n",
        "            )\n",
        "\n",
        "            tfidf_matrix = tfidf.fit_transform([processed])\n",
        "            feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "            # Ordenar por importancia (TF-IDF)\n",
        "            tfidf_scores = zip(feature_names, tfidf_matrix.toarray()[0])\n",
        "            sorted_keywords = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            return [word for word, score in sorted_keywords if score > 0]\n",
        "        except:\n",
        "            # Fallback simple: dividir por espacios y tomar las primeras n palabras\n",
        "            words = processed.split()\n",
        "            return list(set(words))[:n]\n",
        "\n",
        "    def match_cv_with_job(self, cv_file, job_file):\n",
        "        \"\"\"\n",
        "        Realiza el matching completo entre un CV y un puesto\n",
        "\n",
        "        Args:\n",
        "            cv_file: Ruta al CV en PDF\n",
        "            job_file: Ruta a la descripción del puesto en JSON\n",
        "\n",
        "        Returns:\n",
        "            dict: Resultado completo del matching\n",
        "        \"\"\"\n",
        "        # Extraer info del CV\n",
        "        cv_data = self.cv_extractor.process_cv(cv_file)\n",
        "        if \"error\" in cv_data:\n",
        "            return {\"error\": cv_data[\"error\"]}\n",
        "\n",
        "        # Parsear descripción del trabajo\n",
        "        job_data = self.parse_job_description(job_file)\n",
        "        if not job_data:\n",
        "            return {\"error\": \"No se pudo procesar la descripción del trabajo\"}\n",
        "\n",
        "        # Calcular match\n",
        "        match_result = self.calculate_match(cv_data, job_data)\n",
        "\n",
        "        # Encontrar coincidencias de palabras clave\n",
        "        keyword_matches = self.find_keyword_matches(cv_data, job_data)\n",
        "\n",
        "        # Generar sugerencias\n",
        "        suggestions = self.generate_improvement_suggestions(cv_data, job_data, match_result)\n",
        "\n",
        "        # Crear resultado final\n",
        "        result = {\n",
        "            \"titulo_puesto\": job_data[\"titulo\"],\n",
        "            \"match_global\": match_result[\"match_porcentaje\"],\n",
        "            \"match_por_categoria\": match_result[\"match_por_categoria\"],\n",
        "            \"coincidencias_clave\": keyword_matches,\n",
        "            \"sugerencias_mejora\": suggestions\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def batch_process(self, cv_folder, job_file, output_folder):\n",
        "        \"\"\"\n",
        "        Procesa múltiples CVs para un mismo puesto\n",
        "\n",
        "        Args:\n",
        "            cv_folder: Carpeta con CVs en PDF\n",
        "            job_file: Archivo JSON con descripción del puesto\n",
        "            output_folder: Carpeta para guardar resultados\n",
        "        \"\"\"\n",
        "        # Crear carpeta de salida si no existe\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        # Parsear descripción del trabajo\n",
        "        job_data = self.parse_job_description(job_file)\n",
        "        if not job_data:\n",
        "            print(\"Error: No se pudo procesar la descripción del trabajo\")\n",
        "            return\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Procesar todos los CVs\n",
        "        for filename in os.listdir(cv_folder):\n",
        "            if filename.endswith(\".pdf\"):\n",
        "                cv_path = os.path.join(cv_folder, filename)\n",
        "                cv_name = os.path.splitext(filename)[0]\n",
        "\n",
        "                print(f\"Evaluando match para: {filename}\")\n",
        "\n",
        "                # Extraer info del CV\n",
        "                cv_data = self.cv_extractor.process_cv(cv_path)\n",
        "                if \"error\" in cv_data:\n",
        "                    print(f\"Error al procesar {filename}: {cv_data['error']}\")\n",
        "                    continue\n",
        "\n",
        "                # Calcular match\n",
        "                match_result = self.calculate_match(cv_data, job_data)\n",
        "\n",
        "                # Encontrar coincidencias de palabras clave\n",
        "                keyword_matches = self.find_keyword_matches(cv_data, job_data)\n",
        "\n",
        "                # Generar sugerencias\n",
        "                suggestions = self.generate_improvement_suggestions(cv_data, job_data, match_result)\n",
        "\n",
        "                # Agregar resultado individual\n",
        "                result = {\n",
        "                    \"nombre_cv\": cv_name,\n",
        "                    \"titulo_puesto\": job_data[\"titulo\"],\n",
        "                    \"match_global\": match_result[\"match_porcentaje\"],\n",
        "                    \"match_por_categoria\": match_result[\"match_por_categoria\"],\n",
        "                    \"coincidencias_clave\": keyword_matches,\n",
        "                    \"sugerencias_mejora\": suggestions\n",
        "                }\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "                # Guardar resultado individual\n",
        "                output_path = os.path.join(output_folder, f\"{cv_name}_match.json\")\n",
        "                with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(result, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        # Ordenar resultados por porcentaje de match (mayor a menor)\n",
        "        results.sort(key=lambda x: x[\"match_global\"], reverse=True)\n",
        "\n",
        "        # Guardar ranking completo\n",
        "        ranking_path = os.path.join(output_folder, \"ranking_candidatos.json\")\n",
        "        with open(ranking_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                \"puesto\": job_data[\"titulo\"],\n",
        "                \"candidatos\": results\n",
        "            }, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"Proceso de matching completado. Ranking guardado en: {ranking_path}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Función principal\"\"\"\n",
        "    cv_folder = \"cvs/\"\n",
        "    job_file = \"job_description_example.json\"\n",
        "    output_folder = \"resultados_match/\"\n",
        "\n",
        "    # Inicializar matcher\n",
        "    matcher = JobMatcher()\n",
        "\n",
        "    # Procesar todos los CVs contra el puesto especificado\n",
        "    matcher.batch_process(cv_folder, job_file, output_folder)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "cmhdU84TXqXW",
        "outputId": "b2960aec-518f-4557-c507-a73d458c67de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'JobMatcher' object has no attribute 'batch_process'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-4d64949f3093>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-4d64949f3093>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;31m# Procesar todos los CVs contra el puesto especificado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'JobMatcher' object has no attribute 'batch_process'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Cargar archivo\n",
        "with open(\"job_description_example.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    job_data = json.load(f)\n",
        "\n",
        "# Verifica si 'descripcion' existe y no es None\n",
        "descripcion = job_data.get(\"descripcion\")\n",
        "if descripcion is None:\n",
        "    raise ValueError(\"La descripción del trabajo es None o no existe\")\n",
        "\n",
        "# Verifica si 'requisitos' es una lista\n",
        "requisitos = job_data.get(\"requisitos\")\n",
        "if not isinstance(requisitos, list):\n",
        "    raise ValueError(\"Los requisitos deben ser una lista\")\n",
        "\n",
        "# Ejemplo: procesar los requisitos\n",
        "for req in requisitos:\n",
        "    print(\"Requisito:\", req)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx95kLHxY7lv",
        "outputId": "1984fca8-1776-419f-8d77-27cab0691847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requisito: Licenciatura/Grado en Informática, Ingeniería de Software o similar\n",
            "Requisito: Mínimo 3 años de experiencia con Python en entornos de producción\n",
            "Requisito: Experiencia sólida con Django o Flask\n",
            "Requisito: Conocimientos de bases de datos SQL (PostgreSQL, MySQL) y NoSQL (MongoDB)\n",
            "Requisito: Familiaridad con Docker y herramientas de CI/CD\n",
            "Requisito: Experiencia en diseño y consumo de APIs RESTful\n",
            "Requisito: Conocimientos de Git y metodologías ágiles\n"
          ]
        }
      ]
    }
  ]
}